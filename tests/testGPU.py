import tensorflow as tf
import time


gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)  # ë©”ëª¨ë¦¬ ë™ì  í• ë‹¹
        print("âœ… GPU ë©”ëª¨ë¦¬ ë™ì  í• ë‹¹ í™œì„±í™” ì™„ë£Œ!")
    except RuntimeError as e:
        print(f"âŒ GPU ë©”ëª¨ë¦¬ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ëª¨ë¸ ì •ì˜ (ê°„ë‹¨í•œ LSTM)
model = tf.keras.Sequential([
    tf.keras.layers.LSTM(64, input_shape=(10, 1)),
    tf.keras.layers.Dense(10, activation='softmax')
])
model.compile(loss='categorical_crossentropy', optimizer='adam')

# ë”ë¯¸ ë°ì´í„° ìƒì„±
X = tf.random.normal([1000, 10, 1])
y = tf.keras.utils.to_categorical(tf.random.uniform([1000], maxval=10, dtype=tf.int32), num_classes=10)

# CPU ì‹¤í–‰
with tf.device('/CPU:0'):
    start = time.time()
    model.fit(X, y, epochs=1, batch_size=32, verbose=0)
    cpu_time = time.time() - start
    print(f"âš¡ CPU í•™ìŠµ ì‹œê°„: {cpu_time:.2f}ì´ˆ")

# GPU ì‹¤í–‰
with tf.device('/GPU:0'):
    start = time.time()
    model.fit(X, y, epochs=1, batch_size=32, verbose=0)
    gpu_time = time.time() - start
    print(f"ğŸš€ GPU í•™ìŠµ ì‹œê°„: {gpu_time:.2f}ì´ˆ")

# ê²°ê³¼ ë¹„êµ
if gpu_time < cpu_time:
    print("âœ… GPUê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤!")
else:
    print("âŒ GPUê°€ í™œì„±í™”ë˜ì—ˆì§€ë§Œ, í•™ìŠµì´ CPUë³´ë‹¤ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")